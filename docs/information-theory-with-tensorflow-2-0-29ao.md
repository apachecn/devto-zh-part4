# Tensorflow 2.0 ä¿¡æ¯è®º

> åŸæ–‡ï¼š<https://dev.to/mmithrakumar/information-theory-with-tensorflow-2-0-29ao>

ä¿¡æ¯è®ºæ˜¯åº”ç”¨æ•°å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå›´ç»•é‡åŒ–ä¿¡å·ä¸­å­˜åœ¨å¤šå°‘ä¿¡æ¯å±•å¼€ã€‚åœ¨æœºå™¨å­¦ä¹ çš„èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å°†ä¿¡æ¯è®ºåº”ç”¨äºè¿ç»­å˜é‡ï¼Œå…¶ä¸­ä¸€äº›æ¶ˆæ¯é•¿åº¦è§£é‡Šä¸é€‚ç”¨ã€‚

ä¿¡æ¯è®ºèƒŒåçš„åŸºæœ¬ç›´è§‰æ˜¯ï¼Œå¯èƒ½çš„äº‹ä»¶åº”è¯¥å…·æœ‰ä½ä¿¡æ¯é‡ï¼Œä¸å¤ªå¯èƒ½çš„äº‹ä»¶åº”è¯¥å…·æœ‰è¾ƒé«˜ä¿¡æ¯é‡ï¼Œè€Œç‹¬ç«‹äº‹ä»¶åº”è¯¥å…·æœ‰é™„åŠ ä¿¡æ¯ã€‚

è®©æˆ‘ç»™ä½ ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼Œå‡è®¾ä½ æœ‰ä¸€ä¸ªç”·æ€§æœ‹å‹ï¼Œä»–æ·±æ·±åœ°çˆ±ä¸Šäº†è¿™ä¸ªå¥³å­©ï¼Œæ‰€ä»¥ä»–å‡ ä¹æ¯å‘¨éƒ½çº¦è¿™ä¸ªå¥³å­©å‡ºå»ï¼Œæœ‰ 99%çš„å¯èƒ½æ€§å¥¹è¯´ä¸ï¼Œæ‰€ä»¥ä½ æ˜¯ä»–æœ€å¥½çš„æœ‹å‹ï¼Œä»–æ¯æ¬¡çº¦å¥³å­©å‡ºå»åéƒ½ç»™ä½ å‘çŸ­ä¿¡è®©ä½ çŸ¥é“å‘ç”Ÿäº†ä»€ä¹ˆï¼Œä»–å‘çŸ­ä¿¡ï¼Œâ€œå˜¿ï¼ŒçŒœçŒœå¥¹è¯´ä»€ä¹ˆï¼Œä¸ğŸ˜­ğŸ˜­ğŸ˜­â€œï¼Œè¿™å½“ç„¶æ˜¯æµªè´¹ï¼Œ è€ƒè™‘åˆ°ä»–çš„æœºä¼šå¾ˆå°ï¼Œæ‰€ä»¥ä½ çš„æœ‹å‹å‘é€æ›´æœ‰æ„ä¹‰â€ğŸ˜­â€œä½†å¦‚æœå¥¹åŒæ„äº†ï¼Œä»–å½“ç„¶å¯ä»¥å‘ä¸€æ¡æ›´é•¿çš„çŸ­ä¿¡ï¼Œè¿™æ ·ä¸€æ¥ï¼Œç”¨æ¥ä¼ è¾¾ä¿¡æ¯çš„æ¯”ç‰¹æ•°(ä»¥åŠä½ ç›¸åº”çš„æ•°æ®è´¦å•)å°±ä¼šå‡åˆ°æœ€å°‘ã€‚ é™„æ³¨:ä¸è¦å‘Šè¯‰ä½ çš„æœ‹å‹ä»–çš„æœºä¼šå¾ˆå°ï¼Œé‚£ä¼šè®©ä½ å¤±å»æœ‹å‹ğŸ˜¬ã€‚

ä¸ºäº†æ»¡è¶³è¿™äº›æ€§è´¨ï¼Œæˆ‘ä»¬å°†äº‹ä»¶**çš„**è‡ªæˆ‘ä¿¡æ¯**x**= x å®šä¹‰ä¸º:

I(x)=-log P(x)

åœ¨æœ¬ä¹¦ä¸­ï¼Œæˆ‘ä»¬æ€»æ˜¯ä½¿ç”¨ log æ¥è¡¨ç¤ºä»¥ e ä¸ºåº•çš„è‡ªç„¶å¯¹æ•°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯¹ **I(x)** çš„å®šä¹‰æ˜¯ä»¥ **nats** ä¸ºå•ä½ç¼–å†™çš„ã€‚ä¸€ä¸ª nat æ˜¯é€šè¿‡è§‚å¯Ÿæ¦‚ç‡ä¸º 1/e çš„äº‹ä»¶è·å¾—çš„ä¿¡æ¯é‡ã€‚å…¶ä»–æ–‡æœ¬ä½¿ç”¨ä»¥ 2 ä¸ºåº•çš„å¯¹æ•°å’Œç§°ä¸º**ä½**æˆ– **shannons** çš„å•ä½ï¼›ç”¨æ¯”ç‰¹è®¡é‡çš„ä¿¡æ¯åªæ˜¯ç”¨ NAT è®¡é‡çš„ä¿¡æ¯çš„é‡æ–°æ ‡åº¦ã€‚

```
"""
No matter what combination of toss you get the Entropy remains the same but if you change the probability of the
trial, the entropy changes, play around with the probs and see how the entropy is changing and see if the increase
or decrease makes sense.
"""

import tensorflow_probability as tfp
tfd = tfp.distributions

coin_entropy = [0]                                                                     # creating the coin entropy list 
for i in range(10, 11):
    coin = tfd.Bernoulli(probs=0.5)                                                    # Bernoulli distribution
    coin_sample = coin.sample(i)                                                       # we take 1 sample
    coin_entropy.append(coin.entropy())                                                # append the coin entropy
    sns.distplot(coin_entropy, color=color_o, hist=False, kde_kws={"shade": True})     # Plot of the entropy 
print("Entropy of 10 coin tosses in nats: {} \nFor tosses: {}".format(coin_entropy[1], coin_sample))
plt.grid()

Entropy of 10 coin tosses in nats: 0.6931471824645996
For tosses: [0 1 1 1 0 1 1 1 0 1] 
```

Enter fullscreen mode Exit fullscreen mode

[![Entropy Plot](img/11c1ab4c3d683c4f0c6b898b9e3e4d7f.png)](https://res.cloudinary.com/practicaldev/image/fetch/s--TpiGr54k--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://raw.githubusercontent.com/adhiraiyan/DeepLearningWithTF2.0/master/notebooks/figures/ch03/output_104_1.png)

è‡ªæˆ‘ä¿¡æ¯åªå¤„ç†å•ä¸€çš„ç»“æœã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨**é¦™å†œç†µ**æ¥é‡åŒ–æ•´ä¸ªæ¦‚ç‡åˆ†å¸ƒä¸­çš„ä¸ç¡®å®šæ€§:

h(x)= E _(xâˆP)[I(x)]= E _(xâˆP)[log P(x)]

ä¹Ÿè¡¨ç¤ºä¸º **H(P)** ã€‚

åˆ†å¸ƒçš„é¦™å†œç†µæ˜¯ä»è¯¥åˆ†å¸ƒä¸­æå–çš„äº‹ä»¶ä¸­çš„é¢„æœŸä¿¡æ¯é‡ã€‚å®ƒç»™å‡ºäº†å¯¹ä»åˆ†å¸ƒ p ä¸­æå–çš„ç¬¦å·è¿›è¡Œç¼–ç å¹³å‡æ‰€éœ€çš„æ¯”ç‰¹æ•°çš„ä¸‹é™ã€‚æ¥è¿‘ç¡®å®šæ€§çš„åˆ†å¸ƒ(å…¶ä¸­ç»“æœå‡ ä¹æ˜¯ç¡®å®šçš„)å…·æœ‰ä½ç†µï¼›æ›´æ¥è¿‘å‡åŒ€çš„åˆ†å¸ƒå…·æœ‰é«˜ç†µã€‚å½“ **x** è¿ç»­æ—¶ï¼Œé¦™å†œç†µè¢«ç§°ä¸º**å·®ç†µ**ã€‚

```
"""
Note here since we are using the Bernoulli distribution to find the expectation we simply use mean,
if you change the distribution, you need to find the Expectation accordingly
"""

def shannon_entropy_func(p):
    """Calculates the shannon entropy.
    Arguments:
        p (int)        : probability of event.
    Returns:
        shannon entropy.
    """

    return -tf.math.log(p.mean())

# Create a Bernoulli distribution bernoulli_distribution = tfd.Bernoulli(probs=.5)

# Use TFPs entropy method to calculate the entropy of the distribution shannon_entropy = bernoulli_distribution.entropy()

print("TFPs entropy: {} matches with the Shannon Entropy Function we wrote: {}".format(shannon_entropy,
                                                                                       shannon_entropy_func(bernoulli_distribution)))

TFPs entropy: 0.6931471824645996 matches with the Shannon Entropy Function we wrote: 0.6931471824645996 
```

Enter fullscreen mode Exit fullscreen mode

ç†µä¸åœ¨äºå®ƒçš„è§£é‡Šï¼Œè€Œåœ¨äºå®ƒçš„æ€§è´¨ã€‚ä¾‹å¦‚ï¼Œç†µå¹¶ä¸å…³å¿ƒåƒæ–¹å·®è¿™æ ·çš„å®é™… *x* å€¼ï¼Œå®ƒåªè€ƒè™‘å®ƒä»¬çš„æ¦‚ç‡ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬å¢åŠ å–å€¼çš„æ•°é‡ *x* ï¼Œé‚£ä¹ˆç†µå°†å¢åŠ ï¼Œæ¦‚ç‡å°†å˜å¾—ä¸é‚£ä¹ˆé›†ä¸­ã€‚

```
# You can see below by changing the values of x we increase the entropy 
shannon_list = []

for i in range(1, 20):
    uniform_distribution = tfd.Uniform(low=0.0, high=i)    # We create a uniform distribution
    shannon_entropy = uniform_distribution.entropy()       # Calculate the entropy of the uniform distribution
    shannon_list.append(shannon_entropy)                   # Append the results to the list 
# Plot of Shannon Entropy plt.hist(shannon_list, color=color_b)
plt.grid() 
```

Enter fullscreen mode Exit fullscreen mode

[![Shannon Entropy Plot](img/c78694a1b5983e1587c1ea67bae1bb79.png)](https://res.cloudinary.com/practicaldev/image/fetch/s--CUYKSsdJ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://raw.githubusercontent.com/adhiraiyan/DeepLearningWithTF2.0/master/notebooks/figures/ch03/output_108_0.png)

å¦‚æœæˆ‘ä»¬åœ¨åŒä¸€ä¸ªéšæœºå˜é‡ x ä¸Šæœ‰ä¸¤ä¸ªç‹¬ç«‹çš„æ¦‚ç‡åˆ†å¸ƒ P(x)å’Œ Q(x ),æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ **Kullback-Leibler (KL)æ•£åº¦**æ¥æµ‹é‡è¿™ä¸¤ä¸ªåˆ†å¸ƒçš„å·®å¼‚:

d _(KL)(Pâˆ¨Q)= E _(xâˆ¾P)[log P(x)/Q(x)]= E _(xâˆ¾P)[log P(x)-log Q(x)]

åœ¨ç¦»æ•£å˜é‡çš„æƒ…å†µä¸‹ï¼Œå½“æˆ‘ä»¬ä½¿ç”¨è¢«è®¾è®¡ä¸ºæœ€å°åŒ–ä»æ¦‚ç‡åˆ†å¸ƒ q ä¸­æå–çš„æ¶ˆæ¯é•¿åº¦çš„ä»£ç æ—¶ï¼Œå®ƒæ˜¯å‘é€åŒ…å«ä»æ¦‚ç‡åˆ†å¸ƒ P ä¸­æå–çš„ç¬¦å·çš„æ¶ˆæ¯æ‰€éœ€çš„é¢å¤–ä¿¡æ¯é‡

```
def kl_func(p, q):
    """Calculates the KL divergence of two distributions.
    Arguments:
        p    : Distribution p.
        q    : Distribution q.
    Returns:
        the divergence value.
    """

    r = p.loc - q.loc
    return (tf.math.log(q.scale) - tf.math.log(p.scale) -.5 * (1. - (p.scale**2 + r**2) / q.scale**2))

# We create two normal distributions p = tfd.Normal(loc=1., scale=1.)
q = tfd.Normal(loc=0., scale=2.)

# Using TFPs KL Divergence kl = tfd.kl_divergence(p, q)

print("TFPs KL_Divergence: {} matches with the KL Function we wrote: {}".format(kl, kl_func(p, q)))

TFPs KL_Divergence: 0.4431471824645996 matches with the KL Function we wrote: 0.4431471824645996 
```

Enter fullscreen mode Exit fullscreen mode

KL æ•£åº¦æœ‰è®¸å¤šæœ‰ç”¨çš„æ€§è´¨ï¼Œæœ€æ˜æ˜¾çš„æ˜¯éè´Ÿçš„ã€‚å½“ä¸”ä»…å½“ P å’Œ Q åœ¨ç¦»æ•£å˜é‡çš„æƒ…å†µä¸‹æ˜¯ç›¸åŒçš„åˆ†å¸ƒï¼Œæˆ–è€…åœ¨è¿ç»­å˜é‡çš„æƒ…å†µä¸‹â€œå‡ ä¹å¤„å¤„â€ç›¸ç­‰æ—¶ï¼ŒKL æ•£åº¦ä¸º 0ã€‚

ä¸ KL æ•£åº¦å¯†åˆ‡ç›¸å…³çš„ä¸€ä¸ªé‡æ˜¯**äº¤å‰ç†µ** H(Pï¼ŒQ)= H(P)+D _(KL)(Pâˆ¨Q)ï¼Œå®ƒç±»ä¼¼äº KL æ•£åº¦ï¼Œä½†ç¼ºå°‘å·¦è¾¹çš„é¡¹:

H(Pï¼ŒQ)=-E _(xâˆP)log Q(x)

æœ€å°åŒ–å…³äº Q çš„äº¤å‰ç†µç­‰ä»·äºæœ€å°åŒ– KL æ•£åº¦ï¼Œå› ä¸º Q ä¸å‚ä¸è¢«çœç•¥çš„é¡¹ã€‚

```
"""
The cross_entropy computes the Shannons cross entropy defined as:
H[P, Q] = E_p[-log q(X)] = -int_F p(x) log q(x) dr(x)
"""

# We create two normal distributions p = tfd.Normal(loc=1., scale=1.)
q = tfd.Normal(loc=0., scale=2.)

# Calculating the cross entropy cross_entropy = q.cross_entropy(p)

print("TFPs cross entropy: {}".format(cross_entropy))

TFPs cross entropy: 3.418938636779785 
```

Enter fullscreen mode Exit fullscreen mode

* * *

è¿™æ˜¯ã€Šç”¨ Tensorflow 2.0 è¿›è¡Œæ·±åº¦å­¦ä¹ ã€‹ä¸€ä¹¦ã€Šç”¨ Tensorflow 2.0 è¿›è¡Œæ¦‚ç‡å’Œä¿¡æ¯è®ºã€‹ä¸€ç« çš„ç¬¬åä¸‰èŠ‚ã€‚

æ‚¨å¯ä»¥é˜…è¯»æœ¬èŠ‚å’Œä»¥ä¸‹ä¸»é¢˜:

03.00 -æ¦‚ç‡ä¸ä¿¡æ¯è®º
03.01 -ä¸ºä»€ä¹ˆæ˜¯æ¦‚ç‡ï¼Ÿ
03.02 -éšæœºå˜é‡
03.03 -æ¦‚ç‡åˆ†å¸ƒ
03.04 -è¾¹é™…æ¦‚ç‡
03.05 -æ¡ä»¶æ¦‚ç‡
03.06 -æ¡ä»¶æ¦‚ç‡çš„é“¾å¼æ³•åˆ™
03.07 -ç‹¬ç«‹æ€§å’Œæ¡ä»¶ç‹¬ç«‹æ€§
03.08 -æœŸæœ›ã€æ–¹å·®å’Œåæ–¹å·®
03.09 -å¸¸è§æ¦‚ç‡åˆ†å¸ƒ
03.10 -å¸¸ç”¨å‡½æ•°çš„æœ‰ç”¨æ€§è´¨
03.11 -è´å¶æ–¯' 1

åœ¨[ç”¨ TF 2.0 æ·±åº¦å­¦ä¹ :03.00-æ¦‚ç‡è®ºä¸ä¿¡æ¯è®º](https://www.adhiraiyan.org/deeplearning/03.00-Probability-and-Information-Theory)ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œè·å¾—è¿™ç¯‡æ–‡ç« å’Œæœ¬ç« [å…¶ä½™éƒ¨åˆ†çš„ä»£ç ã€‚Google Colab å’Œ Jupyter Binder ä¸­ç¬”è®°æœ¬çš„é“¾æ¥åœ¨](https://github.com/adhiraiyan/DeepLearningWithTF2.0)[ç¬”è®°æœ¬](https://www.adhiraiyan.org/deeplearning/03.00-Probability-and-Information-Theory)çš„æœ«å°¾ã€‚