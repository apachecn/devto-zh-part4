# å¦‚ä½•è¿›å…¥ä¸‹ä¸€é¡µ- 03 - Python scrapy åˆå­¦è€…æ•™ç¨‹

> åŸæ–‡ï¼š<https://dev.to/davidmm1707/how-to-go-to-the-next-page-03-python-scrapy-tutorial-for-beginners-33if>

åŸå¸– [Python Scrapy åˆå­¦è€…æ•™ç¨‹â€“03â€“å¦‚ä½•è¿›å…¥ä¸‹ä¸€é¡µ](https://letslearnabout.net/tutorial/python-scrapy-tutorial-for-beginners-03-how-to-go-to-the-next-page/)

# Python åˆå­¦è€…å‰ªè´´ç°¿æ•™ç¨‹- 03

[![](img/51785e1bd1470c8d322bb51cfd852769.png)](https://res.cloudinary.com/practicaldev/image/fetch/s--0FQ_5-tP--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://i2.wp.com/letslearnabout.net/wp-content/uploads/2019/09/image-8.png%3Fw%3D688%26ssl%3D1)

åœ¨ä¸Šä¸€è¯¾ä¸­ï¼Œ[ç”¨ Scrapy æå–æ‰€æœ‰æ•°æ®ï¼Œ](https://letslearnabout.net/tutorial/python-scrapy-tutorial-for-beginners-02-extract-all-the-data/)æˆ‘ä»¬è®¾æ³•è·å–äº†æ‰€æœ‰ä¹¦ç±çš„ URLï¼Œç„¶åä»æ¯æœ¬ä¹¦ä¸­æå–æ•°æ®ã€‚æˆ‘ä»¬è¢«é™åˆ¶åœ¨ä¸»é¡µä¸Šçš„ä¹¦ç±ï¼Œå› ä¸ºæˆ‘ä»¬ä¸çŸ¥é“å¦‚ä½•ä½¿ç”¨ Scrapy è¿›å…¥ä¸‹ä¸€é¡µã€‚

ç›´åˆ°ç°åœ¨ã€‚

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œä½ å°†å­¦ä¹ å¦‚ä½•:

*   å¯¼èˆªåˆ°â€œä¸‹ä¸€é¡µâ€
*   è§£å†³è·¯ç”±é—®é¢˜
*   æå–æ¯æœ¬ä¹¦çš„æ‰€æœ‰æ•°æ®

[https://www.youtube.com/embed/mDUCyn6pxwQ](https://www.youtube.com/embed/mDUCyn6pxwQ)

* * *

### æˆ‘ä»¬çš„æ¸¸æˆè®¡åˆ’

[![next page using Scrapy - gameplan](img/5cc73cb3ca2842838dfb9b15da70a8cd.png)](https://res.cloudinary.com/practicaldev/image/fetch/s--2rarzD95--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://i0.wp.com/www.hondurassoccer.com/wp-content/uploads/2014/06/pizarra-estrategia.png%3Fw%3D688)

æœ€åˆï¼Œæˆ‘ä»¬åªæ˜¯åˆ—å‡ºæ‰€æœ‰ä¹¦ç±çš„ç½‘å€ï¼Œç„¶åï¼Œä¸€ä¸ªæ¥ä¸€ä¸ªï¼Œæˆ‘ä»¬æå–æ•°æ®ã€‚

å› ä¸ºæˆ‘ä»¬æœ‰ 20 æœ¬ä¹¦ï¼Œæ‰€ä»¥æˆ‘ä»¬åªåˆ—å‡ºäº† 20 æœ¬ä¹¦çš„ URLï¼Œç„¶åè§£æè¿™ 20 ä¸ª URLï¼Œäº§ç”Ÿç»“æœã€‚

æˆ‘ä»¬åªéœ€è¦å†å¢åŠ ä¸€ä¸ªæ­¥éª¤ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬å°†åˆ—å‡º 20 ä¸ªå›¾ä¹¦ URLï¼Œè§£æå®ƒä»¬ï¼Œç„¶åï¼Œå¦‚æœæœ‰â€œä¸‹ä¸€é¡µâ€,æˆ‘ä»¬å°†å¯¼èˆªåˆ°å®ƒä»¥é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œåˆ—å‡ºå¹¶äº§ç”Ÿæ–°çš„ 20 ä¸ªå›¾ä¹¦ URLï¼Œç›´åˆ°æ²¡æœ‰æ›´å¤šçš„é¡µé¢ã€‚

åœ¨æˆ‘ä»¬çš„[ç¾æ±¤æ•™ç¨‹](https://letslearnabout.net/python/beautiful-soup/your-first-web-scraping-script-with-python-beautiful-soup/)ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ç›¸åŒçš„ç­–ç•¥:

[![next page using Scrapy - recursive flow](img/9ea0e674daef2f9a51499558eef56b17.png)](https://res.cloudinary.com/practicaldev/image/fetch/s--i66qgwk1--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://i0.wp.com/letslearnabout.net/wp-content/uploads/2019/08/Untitled-Project.jpg%3Fw%3D688%26ssl%3D1)

è¿™å°±æ˜¯æˆ‘ä»¬ç°åœ¨è¦å¼€å§‹ä½¿ç”¨çš„ã€‚

* * *

### æ£€æŸ¥æ˜¯å¦æœ‰â€œä¸‹ä¸€é¡µâ€

è®©æˆ‘ä»¬ä»ç¬¬äºŒè¯¾ä¸­ä½¿ç”¨çš„ä»£ç å¼€å§‹ï¼Œ[æå–æ‰€æœ‰æ•°æ®](https://letslearnabout.net/tutorial/python-scrapy-tutorial-for-beginners-02-extract-all-the-data/):

```
# -*- coding: utf-8 -*-
import scrapy

class SpiderSpider(scrapy.Spider):
    name = 'spider'
    allowed_domains = ['books.toscrape.com']
    start_urls = ['http://books.toscrape.com/']
    base_url = 'http://books.toscrape.com/'

    def parse(self, response):
        all_books = response.xpath('//article[@class="product_pod"]')

        for book in all_books:
            book_url = self.start_urls[0] + 
                book.xpath('.//h3/a/@href').extract_first()

            yield scrapy.Request(book_url, callback=self.parse_book)

    def parse_book(self, response):
        title = response.xpath('//div/h1/text()').extract_first()

        relative_image = response.xpath('//div[@class="item active"]/img/@src').extract_first()
        final_image = self.base_url + relative_image.replace('../..', '')

        price = response.xpath(
            '//div[contains(@class, "product_main")]/p[@class="price_color"]/text()').extract_first()
        stock = response.xpath(
            '//div[contains(@class, "product_main")]/p[contains(@class, "instock")]/text()').extract()[1].strip()
        stars = response.xpath(
            '//div/p[contains(@class, "star-rating")]/@class').extract_first().replace('star-rating ', '')
        description = response.xpath(
            '//div[@id="product_description"]/following-sibling::p/text()').extract_first()
        upc = response.xpath(
            '//table[@class="table table-striped"]/tr[1]/td/text()').extract_first()
        price_excl_tax = response.xpath(
            '//table[@class="table table-striped"]/tr[3]/td/text()').extract_first()
        price_inc_tax = response.xpath(
            '//table[@class="table table-striped"]/tr[4]/td/text()').extract_first()
        tax = response.xpath(
            '//table[@class="table table-striped"]/tr[5]/td/text()').extract_first()

        yield {
            'Title': title,
            'Image': final_image,
            'Price': price,
            'Stock': stock,
            'Stars': stars,
            'Description': description,
            'Upc': upc,
            'Price after tax': price_excl_tax,
            'Price incl tax': price_inc_tax,
            'Tax': tax,
        } 
```

ç”±äºè¿™ç›®å‰æ­£åœ¨å·¥ä½œï¼Œæˆ‘ä»¬åªéœ€è¦åœ¨ for å¾ªç¯ç»“æŸåæ£€æŸ¥æ˜¯å¦æœ‰â€œä¸‹ä¸€æ­¥â€æŒ‰é’®ã€‚å³é”®å•å‡»ä¸‹ä¸€æ­¥æŒ‰é’®:

[![](img/6719b3707131986d268fd03319c46a61.png)](https://res.cloudinary.com/practicaldev/image/fetch/s--mYK_Wn8e--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://i0.wp.com/letslearnabout.net/wp-content/uploads/2019/09/image-9.png%3Ffit%3D688%252C398%26ssl%3D1)

ä¸‹ä¸€é¡µçš„ URL åœ¨ä¸€ä¸ª*ä¸€ä¸ª*æ ‡ç­¾é‡Œé¢ï¼Œåœ¨ä¸€ä¸ª*æ*æ ‡ç­¾é‡Œé¢ã€‚ä½ çŸ¥é“å¦‚ä½•æå–å®ƒï¼Œæ‰€ä»¥åˆ›å»ºä¸€ä¸ª _next_page_url _ æˆ‘ä»¬å¯ä»¥å¯¼èˆªåˆ°ã€‚è¯·æ³¨æ„ï¼Œå®ƒæ˜¯ä¸€ä¸ªéƒ¨åˆ† URLï¼Œæ‰€ä»¥æ‚¨éœ€è¦æ·»åŠ åŸºæœ¬ URLã€‚å°±åƒæˆ‘ä»¬ä»¥å‰åšçš„é‚£æ ·ï¼Œä½ å¯ä»¥è‡ªå·±åšã€‚è¯•è¯•çœ‹ã€‚

æˆ‘æ˜¯è¿™æ ·åšçš„:

```
 for book in all_books:
            book_url = self.start_urls[0] + 
                book.xpath('.//h3/a/@href').extract_first()

            yield scrapy.Request(book_url, callback=self.parse_book)

        # New code:
        next_page_partial_url = response.xpath(
            '//li[@class="next"]/a/@href').extract_first()

        next_page_url = self.base_url + next_page_partial_url
        yield scrapy.Request(next_page_url, callback=self.parse) 
```

ç”¨*scrapy crawl spider-o next _ page . JSON*è¿è¡Œä»£ç ï¼Œæ£€æŸ¥ç»“æœã€‚

è¿™æ˜¯æ€ä¹ˆå›äº‹ï¼Ÿæ–‡ä»¶ä¸­åªæœ‰ 20 ä¸ªå…ƒç´ ï¼è®©æˆ‘ä»¬æ£€æŸ¥æ—¥å¿—ï¼Œçœ‹çœ‹å‘ç”Ÿäº†ä»€ä¹ˆäº‹ã€‚

[![](img/ead79ddb9fa70bf8ff9e8b12dd03f27e.png)](https://res.cloudinary.com/practicaldev/image/fetch/s--j9XG-OLf--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://i2.wp.com/letslearnabout.net/wp-content/uploads/2019/09/image-10.png%3Ffit%3D688%252C269%26ssl%3D1)

æˆ‘ä»¬è®¾æ³•å¼„åˆ°äº†ç¬¬ä¸€æ‰¹ 20 æœ¬ä¹¦ï¼Œä½†æ˜¯çªç„¶ä¹‹é—´ï¼Œæˆ‘ä»¬å†ä¹Ÿä¹°ä¸åˆ°æ›´å¤šçš„ä¹¦äº†â€¦

*books.toscrape.com*æ˜¯ç”±[æŠ“å–ä¸­å¿ƒ](https://scrapinghub.com/)åˆ¶ä½œçš„ä¸€ä¸ªç½‘ç«™ï¼Œç”¨æ¥è®­ç»ƒäººä»¬æŠ“å–ç½‘é¡µï¼Œä»–ä»¬æœ‰ä¸€äº›ä½ éœ€è¦æ³¨æ„çš„å°é™·é˜±ã€‚æ¯”è¾ƒæˆåŠŸçš„ URL(è“è‰²ä¸‹åˆ’çº¿)å’Œå¤±è´¥çš„ URL(çº¢è‰²ä¸‹åˆ’çº¿)ã€‚æ¯ä¸ªè·¯ç”±ä¸Šéƒ½æœ‰ _/catalogue _missingã€‚ä»–ä»¬æ·»åŠ å®ƒä¸æ˜¯ä¸ºäº†è®©ä½ å¤±è´¥ã€‚

è®©æˆ‘ä»¬è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

* * *

### è§£å†³â€˜ä¹¦â€™çš„è·¯ç”±é—®é¢˜

ç”±äºæœ‰äº› URL ç¼ºå°‘*/ç›®å½•*ï¼Œæˆ‘ä»¬æ¥æ£€æŸ¥ä¸€ä¸‹:å¦‚æœè·¯ç”±æ²¡æœ‰ï¼Œæˆ‘ä»¬å°±æŠŠå®ƒåŠ åˆ°éƒ¨åˆ† URL çš„å‰ç¼€ä¸Šã€‚å°±è¿™ä¹ˆç®€å•ã€‚

ç»§ç»­ä¹‹å‰ï¼Œè¯·è‡ªè¡Œå°è¯•ã€‚ä½ å¯ä»¥åœ¨è¿™é‡ŒæŸ¥çœ‹æˆ‘çš„ä»£ç :

```
 for book in all_books:
            book_url = book.xpath('.//h3/a/@href').extract_first()

            if 'catalogue/' not in book_url:
                book_url = 'catalogue/' + book_url

            book_url = self.base_url + book_url 
```

è®©æˆ‘ä»¬å†æ¬¡è¿è¡Œä»£ç ï¼åº”è¯¥ç®¡ç”¨å§ï¼Ÿ*scrapy crawl spider-o next _ page . JSON*

ç°åœ¨æˆ‘ä»¬æœ‰æ›´å¤šçš„ä¹¦äº†ï¼ä½†åªæœ‰ 40 å²ã€‚æˆ‘ä»¬è®¾æ³•å¾—åˆ°äº†ç¬¬ä¸€ä¸ª 20ï¼Œç„¶åæ˜¯ä¸‹ä¸€ä¸ª 20ã€‚ç„¶åï¼Œå‘ç”Ÿäº†ä¸€ä»¶äº‹ã€‚æˆ‘ä»¬æ²¡æœ‰ä»ç¬¬äºŒé¡µæ‹¿åˆ°ç¬¬ä¸‰é¡µã€‚è®©æˆ‘ä»¬è½¬åˆ°ç¬¬äºŒé¡µï¼Œçœ‹çœ‹â€œä¸‹ä¸€æ­¥â€æŒ‰é’®æ˜¯æ€ä¹ˆå›äº‹ï¼Œå¹¶å°†å…¶ä¸ç¬¬ä¸€ä¸ªæŒ‰é’®(åŠå…¶åˆ°ç¬¬äºŒä¸ªæŒ‰é’®çš„é“¾æ¥)è¿›è¡Œæ¯”è¾ƒ

[![](img/3a3f6c9fc7b9f368365790ffdc2b75f4.png)](https://res.cloudinary.com/practicaldev/image/fetch/s--qa74aM2W--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://i1.wp.com/letslearnabout.net/wp-content/uploads/2019/09/image-11.png%3Ffit%3D688%252C851%26ssl%3D1)

æˆ‘ä»¬é‡åˆ°äº†ä¸ä¹¦ç±ç›¸åŒçš„é—®é¢˜:æœ‰äº›é“¾æ¥æœ‰*/ç›®å½•*ï¼Œæœ‰äº›æ²¡æœ‰ã€‚

* * *

### è§£å†³â€˜ä¸‹ä¸€ä¸ªâ€™è·¯ç”±é—®é¢˜

å› ä¸ºæˆ‘ä»¬æœ‰åŒæ ·çš„é—®é¢˜ï¼Œæ‰€ä»¥æˆ‘ä»¬æœ‰åŒæ ·çš„è§£å†³æ–¹æ¡ˆã€‚ä¸€ä¸ªä½ å¯ä»¥è½»æ¾è§£å†³çš„é—®é¢˜ã€‚ä½ ä¸ºä»€ä¹ˆä¸è¯•è¯•ï¼ŸåŒæ ·ï¼Œä½ åªéœ€è¦æ£€æŸ¥é“¾æ¥å’Œå‰ç¼€*/ç›®å½•*ï¼Œä»¥é˜²å­å­—ç¬¦ä¸²ä¸å­˜åœ¨ã€‚

å¦‚æœä½ ä¸èƒ½è§£å†³å®ƒï¼Œè¿™æ˜¯æˆ‘çš„è§£å†³æ–¹æ¡ˆ:

```
next_page_partial_url = response.xpath(
            '//li[@class="next"]/a/@href').extract_first()

        if next_page_partial_url:
            if 'catalogue/' not in next_page_partial_url:
                next_page_partial_url = "catalogue/" + next_page_partial_url

            next_page_url = self.base_url + next_page_partial_url
            yield scrapy.Request(next_page_url, callback=self.parse) 
```

æ‚¨å¯ä»¥çœ‹åˆ°è¿™ç§æ¨¡å¼:æˆ‘ä»¬è·å–éƒ¨åˆ† URLï¼Œæ£€æŸ¥æ˜¯å¦ç¼ºå°‘*/catalog*ï¼Œå¦‚æœç¼ºå°‘ï¼Œæˆ‘ä»¬æ·»åŠ å®ƒã€‚ç„¶åï¼Œæˆ‘ä»¬æ·»åŠ  base_urlï¼Œæˆ‘ä»¬å°±æœ‰äº†æˆ‘ä»¬çš„ç»å¯¹ urlã€‚

å†æ¬¡è¿è¡Œèœ˜è››: *scrapy æŠ“å–èœ˜è››-o next_page.json* ã€‚

ç°åœ¨æˆ‘ä»¬æœ‰ 1000 æœ¬ä¹¦äº†ã€‚æ¯ä¸€ä¸ªéƒ½æ˜¯ã€‚ğŸ™‚

è¿™æ˜¯æœ€åçš„ä»£ç :

```
# -*- coding: utf-8 -*-
import scrapy

class SpiderSpider(scrapy.Spider):
    name = 'spider'
    allowed_domains = ['books.toscrape.com']
    start_urls = ['http://books.toscrape.com/']
    base_url = 'http://books.toscrape.com/'

    def parse(self, response):
        all_books = response.xpath('//article[@class="product_pod"]')

        for book in all_books:
            book_url = book.xpath('.//h3/a/@href').extract_first()

            if 'catalogue/' not in book_url:
                book_url = 'catalogue/' + book_url

            book_url = self.base_url + book_url

            yield scrapy.Request(book_url, callback=self.parse_book)

        next_page_partial_url = response.xpath(
            '//li[@class="next"]/a/@href').extract_first()

        if next_page_partial_url:
            if 'catalogue/' not in next_page_partial_url:
                next_page_partial_url = "catalogue/" + next_page_partial_url

            next_page_url = self.base_url + next_page_partial_url
            yield scrapy.Request(next_page_url, callback=self.parse)

    def parse_book(self, response):
        title = response.xpath('//div/h1/text()').extract_first()

        relative_image = response.xpath(
            '//div[@class="item active"]/img/@src').extract_first()
        final_image = self.base_url + relative_image.replace('../..', '')

        price = response.xpath(
            '//div[contains(@class, "product_main")]/p[@class="price_color"]/text()').extract_first()
        stock = response.xpath(
            '//div[contains(@class, "product_main")]/p[contains(@class, "instock")]/text()').extract()[1].strip()
        stars = response.xpath(
            '//div/p[contains(@class, "star-rating")]/@class').extract_first().replace('star-rating ', '')
        description = response.xpath(
            '//div[@id="product_description"]/following-sibling::p/text()').extract_first()
        upc = response.xpath(
            '//table[@class="table table-striped"]/tr[1]/td/text()').extract_first()
        price_excl_tax = response.xpath(
            '//table[@class="table table-striped"]/tr[3]/td/text()').extract_first()
        price_inc_tax = response.xpath(
            '//table[@class="table table-striped"]/tr[4]/td/text()').extract_first()
        tax = response.xpath(
            '//table[@class="table table-striped"]/tr[5]/td/text()').extract_first()

        yield {
            'Title': title,
            'Image': final_image,
            'Price': price,
            'Stock': stock,
            'Stars': stars,
            'Description': description,
            'Upc': upc,
            'Price after tax': price_excl_tax,
            'Price incl tax': price_inc_tax,
            'Tax': tax,
        } 
```

* * *

### ç»“è®º

ä½ ä»Šå¤©è¾¾åˆ°äº†ä¸€ä¸ªé‡Œç¨‹ç¢‘ã€‚ç°åœ¨ä½ å¯ä»¥ä»ä¸€ä¸ªç½‘ç«™ä¸­æå–æ¯ä¸€ä¸ªå…ƒç´ ã€‚

ä½ å·²ç»çŸ¥é“äº†ä½ éœ€è¦å¾—åˆ°ç¬¬ä¸€é¡µä¸Šçš„æ‰€æœ‰å…ƒç´ ï¼Œå•ç‹¬åœ°åˆ é™¤å®ƒä»¬ï¼Œä»¥åŠå¦‚ä½•è½¬åˆ°ä¸‹ä¸€é¡µé‡å¤è¿™ä¸ªè¿‡ç¨‹ã€‚è®©æˆ‘å†ä¸€æ¬¡å±•ç¤ºè¿™å¼ å›¾è¡¨:

[![next page using Scrapy - Recursive flow](img/9ea0e674daef2f9a51499558eef56b17.png)](https://res.cloudinary.com/practicaldev/image/fetch/s--i66qgwk1--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://i0.wp.com/letslearnabout.net/wp-content/uploads/2019/08/Untitled-Project.jpg%3Fw%3D688%26ssl%3D1)

ä¸ä»…å¦‚æ­¤ã€‚è¿™ä¸ªä¾‹å­å¾ˆæ£˜æ‰‹ï¼Œå› ä¸ºæˆ‘ä»¬å¿…é¡»æ£€æŸ¥éƒ¨åˆ† URL æ˜¯å¦æœ‰*/catalog*æ¥æ·»åŠ å®ƒã€‚

é€šå¸¸æƒ…å†µä¸‹ï¼Œç”¨ Scrapy å¯¹ç½‘ç«™è¿›è¡Œåˆ†é¡µæ›´å®¹æ˜“ï¼Œå› ä¸ºâ€œä¸‹ä¸€æ­¥â€æŒ‰é’®åŒ…å«å®Œæ•´çš„ URLï¼Œæ‰€ä»¥è¿™ä¸ªä¾‹å­æ¯”æ­£å¸¸æƒ…å†µä¸‹æ›´éš¾ï¼Œä½†ä½ è¿˜æ˜¯æˆåŠŸäº†ï¼

ä½†æ˜¯â€¦å¦‚æœæˆ‘å‘Šè¯‰ä½ è¿™æ¯”æˆ‘ä»¬ä»¥å‰åšçš„æ›´å®¹æ˜“å‘¢ï¼Ÿ

ä¸å…¶æ‹¿ç€ä½ çš„å¹²è‰å‰ç›´å¥”æˆ‘å®¶ï¼Œä¸å¦‚å»å‚åŠ ç¬¬å››è¯¾ï¼Œåœ¨é‚£é‡Œä½ å°†å­¦ä¼šå¦‚ä½•ä½¿ç”¨çˆ¬è¡Œå™¨ä»¥æ›´ç®€å•çš„æ–¹å¼åˆ®æ¯ä¸€ä»¶ç‰©å“ã€‚

* * *

[æˆ‘çš„ Youtube æ•™ç¨‹è§†é¢‘](https://www.youtube.com/channel/UC9OLm6YFRzr4yjlw4xNWYvg?sub_confirmation=1)

[Github ä¸Šçš„æœ€ç»ˆä»£ç ](https://github.com/david1707/scrapy_tutorial/tree/03_lesson)

[åœ¨ Twitter ä¸Šè”ç³»æˆ‘](https://twitter.com/DavidMM1707)

[ä¸Šä¸€è¯¾:02â€“åˆ›å»ºæ‚¨çš„ç¬¬ä¸€ä¸ªèœ˜è››](https://letslearnabout.net/tutorial/python-scrapy-tutorial-for-beginners-02-extract-all-the-data/)